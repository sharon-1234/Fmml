{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sharon-1234/Fmml/blob/main/FMML_Module_9_Lab_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 9: Convolutional Neural Networks\n",
        "## **Lab 3**\n",
        "### Module coordinator: Kushagra Agarwal"
      ],
      "metadata": {
        "id": "kCpbL40ggQf1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Understanding Convolutions"
      ],
      "metadata": {
        "id": "0hAW8ptqVeyP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://miro.medium.com/max/464/0*e-SMFTzO8r7skkpc\" width=650px/>"
      ],
      "metadata": {
        "id": "q6wfvhccKxWx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yZD5S7IQgHbU"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing some pytorch packages\n",
        "import torch\n",
        "from torch.nn import Conv2d"
      ],
      "metadata": {
        "id": "BDE4WBHalreb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Central to CNNs, a convolution operation is a linear element-wise multiplication operation between a small filter/kernel and same-sized patch from the image. We move this filter over the image like a sliding window from top left to bottom right. For each point on the image, a value is calculated based on the filter using a convolution operation. These filters can do simplest task like checking if there is a vertical line in the image or complicated task like detecting a human eye in the image.\n",
        "\n",
        "Let's look at the convolution formula:\n",
        "\n",
        "Convolution between image\n",
        "$f(x, y)$ and kernel $k(x, y)$ is\n",
        "$$f(x,y) * k(x,y) = \\sum \\limits _{i=0} ^{W-1} \\sum \\limits _{j=0} ^{H-1} f(i, j) k(x − i, y − j)$$\n",
        "\n",
        "where $W$ and $H$ are the the width and height of the image.\n",
        "\n",
        "The code demonstrates the convolution operation of a 2D matrix (image) with various filters"
      ],
      "metadata": {
        "id": "hbpRXyTpVv7u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://www.researchgate.net/profile/Chaim-Baskin/publication/318849314/figure/fig1/AS:614287726870532@1523469015098/Image-convolution-with-an-input-image-of-size-7-7-and-a-filter-kernel-of-size-3-3.png\" alt=\"Convolution\" width=650px height=280px/>"
      ],
      "metadata": {
        "id": "amI6DTS0Ksvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2D 3x3 binary image with vertical edge\n",
        "image1 = np.array([[1,1,0], [1,1,0], [1,1,0]])\n",
        "\n",
        "# 2D 3x3 binary image with horizontal edge\n",
        "image2 = np.array([[0,0,0], [0,0,0], [1,1,1]])\n",
        "\n",
        "# On plotting the images\n",
        "plt.imshow(image1, cmap='gray', extent=[0, 3, 3, 0])\n",
        "plt.show()\n",
        "plt.imshow(image2, cmap='gray', extent=[0, 3, 3, 0])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        },
        "id": "IalqupPPkDil",
        "outputId": "205d6933-8248-4dfe-bc41-9a90989f2742"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vertical Line filter\n",
        "filter = np.array([[1,0,-1],\n",
        "                   [1,0,-1],\n",
        "                   [1,0,-1]])\n",
        "\n",
        "# Applying filter to first image\n",
        "output = np.sum(np.multiply(image1, filter))\n",
        "print('Output from first image: ', output)\n",
        "\n",
        "# Applying filter to second image\n",
        "output = np.sum(np.multiply(image2, filter))\n",
        "print('Output from second image: ', output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g42INjCaketK",
        "outputId": "843736f5-5ac0-40c9-8f30-c5b7d731ac31"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output from first image:  3\n",
            "Output from second image:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Horizontal edge filter\n",
        "filter = np.array([[-1,-1,-1],\n",
        "                   [ 0, 0, 0],\n",
        "                   [ 1, 1, 1]])\n",
        "\n",
        "output = np.sum(np.multiply(image1, filter))\n",
        "print('Output from first image: ', output)\n",
        "\n",
        "output = np.sum(np.multiply(image2, filter))\n",
        "print('Output from second image: ', output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tba3ySYUk2df",
        "outputId": "50ed05ee-85d9-4d2e-bd18-0f3d34a7f3e2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output from first image:  0\n",
            "Output from second image:  3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Non-zero output suggests that there is a vertical edge present in the first image and not present in the second image. Similarly, horizontal edge is detected in second."
      ],
      "metadata": {
        "id": "BmYcPhDgk_in"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define a function to use convolution layer from Pytorch and use our own kernel to detect edges in image"
      ],
      "metadata": {
        "id": "UNdrDtAKqyj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_conv(image, kernel, padding=0, stride=1):\n",
        "\n",
        "  #--------IMAGE PREPROCESSING-------\n",
        "  image = torch.from_numpy(image)\n",
        "  # Pytorch requires input to convolution in (N,C,H,W), where N = batch size and C=#channels in input\n",
        "  input = image.view((1,1,image.shape[0], image.shape[1]))\n",
        "\n",
        "  # --------------KERNEL-------------\n",
        "  kernel = torch.from_numpy(kernel.astype(np.float32))\n",
        "\n",
        "  # Pytorch requires kernel of shape (N,C,H,W), where N = batch size and C=#channels in input\n",
        "  kernel = kernel.view((1,1,kernel.shape[0], kernel.shape[1]))\n",
        "\n",
        "  # ---------CONVOLUTION LAYER from Pytorch--------\n",
        "  conv = Conv2d(in_channels=1, out_channels=1, kernel_size=kernel.shape, padding=padding, stride=stride)\n",
        "\n",
        "  # Set the kernel weights in the convolution layer\n",
        "  conv.weight = torch.nn.Parameter(kernel)\n",
        "\n",
        "  # ---------APPLY CONVOLUTION--------\n",
        "  output = conv(input.float())\n",
        "  output_img = output.data.numpy()  # Tensor to back in numpy\n",
        "  output_img = output_img.reshape((-1, output_img.shape[-1])) # Reshape to 2D image\n",
        "\n",
        "  return output_img"
      ],
      "metadata": {
        "id": "G5fRJziBk3YB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Our original lotus image\n",
        "image = cv2.imread('/content/grid1 (1).jpg', 0)\n",
        "\n",
        "filter = np.array([[-1,-1,-1],\n",
        "                   [ 0, 0, 0],\n",
        "                   [ 1, 1, 1]])\n",
        "\n",
        "out1 = apply_conv(image, filter, padding=0, stride=1)\n",
        "\n",
        "filter = np.array([[1,0,-1],\n",
        "                   [1,0,-1],\n",
        "                   [1,0,-1]])\n",
        "\n",
        "out2 = apply_conv(image, filter, padding=0, stride=1)"
      ],
      "metadata": {
        "id": "1HPV6fFZloyc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "49342f43-541d-4f41-87cb-940444437972"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "expected np.ndarray (got NoneType)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-53b9a77372c3>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                    [ 1, 1, 1]])\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mout1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m filter = np.array([[1,0,-1],\n",
            "\u001b[0;32m<ipython-input-6-c272bd410f3f>\u001b[0m in \u001b[0;36mapply_conv\u001b[0;34m(image, kernel, padding, stride)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;31m#--------IMAGE PREPROCESSING-------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0;31m# Pytorch requires input to convolution in (N,C,H,W), where N = batch size and C=#channels in input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected np.ndarray (got NoneType)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(10,6))\n",
        "ax = fig.add_subplot(1,3,1)\n",
        "ax.imshow(image, cmap='gray')\n",
        "ax.set_title('Original Image')\n",
        "ax = fig.add_subplot(1,3,2)\n",
        "ax.set_title('Horizontal edge')\n",
        "ax.imshow(out1, cmap='gray')\n",
        "ax = fig.add_subplot(1,3,3)\n",
        "ax.imshow(out2, cmap='gray')\n",
        "ax.set_title('Vertical edge')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xgwXwbUKnmEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pooling Layers\n",
        "\n",
        "Pooling layers are used to reduce the dimensions of the feature maps. Thus, it reduces the number of parameters to learn and the amount of computation performed in the network. The pooling layer summarises the features present in a region of the feature map generated by a convolution layer.\n",
        "\n",
        "1) Max Pooling:\n",
        "\n",
        "<img src='https://media.geeksforgeeks.org/wp-content/uploads/20190721025744/Screenshot-2019-07-21-at-2.57.13-AM.png' height=150px/>\n",
        "\n",
        "2) Average Pooling:\n",
        "\n",
        "<img src='https://media.geeksforgeeks.org/wp-content/uploads/20190721030705/Screenshot-2019-07-21-at-3.05.56-AM.png' height=150px/>"
      ],
      "metadata": {
        "id": "FpA0yEk1BgRb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Softmax layer/activation\n",
        "Recall that logistic regression produces a decimal between 0 and 1.0. For example, a logistic regression output of 0.8 from an email classifier suggests an 80% chance of an email being spam and a 20% chance of it being not spam. Clearly, the sum of the probabilities of an email being either spam or not spam is 1.0.\n",
        "\n",
        "Softmax extends this idea into a multi-class world. That is, Softmax assigns decimal probabilities to each class in a multi-class problem. Those decimal probabilities must add up to 1.0. This additional constraint helps training converge more quickly than it otherwise would.\n",
        "Softmax is implemented through a neural network layer just before the output layer. The Softmax layer must have the same number of nodes as the output layer.\n",
        "\n",
        "<img src='https://miro.medium.com/max/1400/1*ReYpdIZ3ZSAPb2W8cJpkBg.jpeg' height=170px />"
      ],
      "metadata": {
        "id": "eu3QIU7AEO_x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning to train a CNN network"
      ],
      "metadata": {
        "id": "P6grxC0TKKSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import packages\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "qlO-uZUHnn_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Images returned from torchvision dataset classes is in range [0,1]\n",
        "# We transform them to tensors and normalize them to range [-1,1] using 'Normalize' transform\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "# Classes in CIFAR10\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "id": "NnezCUbwGqzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Training data shape : ', trainset.data.shape, len(trainset.targets))\n",
        "print('Testing data shape : ', testset.data.shape, len(testset.targets))\n",
        "\n",
        "# Find the unique numbers from the train labels\n",
        "nClasses = len(classes)\n",
        "print('Total number of outputs : ', nClasses)\n",
        "print('Output classes : ', classes)"
      ],
      "metadata": {
        "id": "e2M57DhHGupn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(num_epochs, model, train_loader, loss_func, optimizer):\n",
        "\n",
        "  # Training mode\n",
        "  model.train()\n",
        "\n",
        "  train_losses = []\n",
        "  train_acc = []\n",
        "\n",
        "  # Train the model\n",
        "  for epoch in range(num_epochs):\n",
        "    running_loss = 0\n",
        "    running_acc = 0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "      # clear gradients for this training step\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Forward pass\n",
        "      output = model(images)\n",
        "\n",
        "      # Calculate loss\n",
        "      loss = loss_func(output, labels)\n",
        "\n",
        "      # Backpropagation, compute gradients\n",
        "      loss.backward()\n",
        "\n",
        "      # Apply gradients\n",
        "      optimizer.step()\n",
        "\n",
        "      # Running loss\n",
        "      running_loss += loss.item()\n",
        "\n",
        "      # indices of max probabilities\n",
        "      _, preds = torch.max(output, dim=1)\n",
        "\n",
        "      # Calculate number of correct predictions\n",
        "      correct = (preds.float() == labels).sum()\n",
        "      running_acc += correct\n",
        "\n",
        "      # Average loss and acc values\n",
        "      epoch_loss = running_loss / len(train_loader.dataset)\n",
        "      epoch_acc = running_acc / len(train_loader.dataset)\n",
        "\n",
        "    train_losses.append(epoch_loss)\n",
        "    train_acc.append(epoch_acc)\n",
        "    print ('Epoch {}/{}, Loss: {:.4f}, Accuracy: {:.4f}'.format(epoch + 1, num_epochs, epoch_loss, epoch_acc*100))\n",
        "\n",
        "  return train_losses, train_acc"
      ],
      "metadata": {
        "id": "_haw697lHCZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, testloader):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  # Deactivate autograd engine (don't compute grads since we're not training)\n",
        "  with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        # Calculate outputs by running images through the network\n",
        "        outputs = model(images)\n",
        "        # The class with the highest value is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print('Accuracy of the network: %d %%' % (\n",
        "      100 * correct / total))"
      ],
      "metadata": {
        "id": "x1Wi6vW7IHcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN with 2 CONV layers and 3 FC layers\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
        "        self.fc1 = nn.Linear(32 * 5 * 5, 512)\n",
        "        self.fc2 = nn.Linear(512, 128)\n",
        "        # output layer 10 classes\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        # flatten all dimensions except batch\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "RgxbRadcHIms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "print(model)"
      ],
      "metadata": {
        "id": "02meBxVOHLNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross Entropy loss for multi-class classification\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "lfKHypeYHNHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SGD optimizer with momentum\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.9)"
      ],
      "metadata": {
        "id": "MuDnJL28HPKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5  # iterations\n",
        "train_losses, train_acc = train(num_epochs, model, trainloader, criterion, optimizer)"
      ],
      "metadata": {
        "id": "AgKhwMrtHRCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(10,4))\n",
        "ax = fig.add_subplot(1,2, 1)\n",
        "ax.plot(np.arange(1,len(train_losses)+1),train_losses)\n",
        "plt.xlabel('Training loss')\n",
        "plt.ylabel('Epochs')\n",
        "ax.set_title('Loss vs Epochs')\n",
        "ax = fig.add_subplot(1,2, 2)\n",
        "ax.plot(np.arange(1,len(train_acc)+1),train_acc)\n",
        "plt.xlabel('Training accuracy')\n",
        "plt.ylabel('Epochs')\n",
        "ax.set_title('Accuracy vs Epochs')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tM2wHKGuHToB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy on test data after training\n",
        "def test_model(model, testloader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Accuracy on test data: {accuracy:.2f}%\")\n",
        "\n",
        "# Assuming you have a PyTorch model and testloader ready\n",
        "test_model(model, testloader)\n"
      ],
      "metadata": {
        "id": "3sHK9hhmI-VY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "615840bc-c7a9-498c-c2c8-9a6a317f5795"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-e4e4d8b247ab>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Assuming you have a PyTorch model and testloader ready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions\n",
        "1) List some reasons why we should prefer CNN over ANN for image classification?\n",
        "\n",
        "2) Try improving the CNN performance further by tuning the hyperparameters(epochs, optimizer, LR etc). Report the improved test accuracy.\n",
        "\n",
        "3) What happens if you reduce the number of convolution layers to only 1?\n",
        "\n",
        "4) Why didn't we use the Softmax activation in the last layer of CNN?\n"
      ],
      "metadata": {
        "id": "RBQeCEB6REnH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GnyOxVlLdnXV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ANSWERS:\n",
        "\n",
        "1)ANSWER:-\n",
        "CNNs (Convolutional Neural Networks) are preferred over ANNs (Artificial Neural Networks) for image classification tasks due to several reasons:\n",
        "\n",
        "1. **Spatial Hierarchies of Features**: CNNs are designed to capture spatial hierarchies of features in images. They leverage convolutional layers to detect low-level features like edges and textures and then progressively combine them to detect higher-level features like shapes and objects. This hierarchical feature extraction is crucial for understanding complex visual patterns.\n",
        "\n",
        "2. **Parameter Sharing**: CNNs utilize parameter sharing, where a single set of learnable parameters (filters) is applied across different spatial locations in the input image. This reduces the number of parameters in the model, making it more efficient and easier to train, especially with large images.\n",
        "\n",
        "3. **Translation Invariance**: CNNs inherently possess translation invariance, meaning they can recognize objects regardless of their position in the image. This is achieved through the use of convolutional and pooling layers, which extract features irrespective of their location in the input.\n",
        "\n",
        "4. **Local Connectivity**: CNNs capture local dependencies in images by connecting each neuron to only a small local region of the input volume. This local connectivity enables the model to focus on relevant features within the receptive field, leading to better generalization and robustness to variations in input.\n",
        "\n",
        "5. **Feature Reuse**: CNNs learn to reuse learned features across the image, making them more data-efficient. This is particularly beneficial for tasks with limited training data, as the model can leverage learned representations from one part of the image to understand similar patterns in other parts.\n",
        "\n",
        "6. **Pooling Layers**: CNNs often include pooling layers that downsample the feature maps, reducing their spatial dimensions while retaining important information. Pooling helps to make the model more robust to variations in input and reduces overfitting by introducing a form of regularization.\n",
        "\n",
        "7. **Availability of Pre-trained Models**: Due to their effectiveness in image classification tasks, many pre-trained CNN models are available, such as VGG, ResNet, and MobileNet. These pre-trained models, trained on large-scale image datasets like ImageNet, can be fine-tuned or used as feature extractors for various downstream tasks, saving time and computational resources.\n",
        "\n",
        "8. **Hardware Optimization**: CNNs are highly amenable to parallelization, making them suitable for efficient implementation on specialized hardware like GPUs and TPUs. This allows for faster training and inference, essential for real-time applications and large-scale deployments.\n",
        "\n",
        "Overall, the unique architectural characteristics of CNNs make them the preferred choice for image classification tasks, offering superior performance, efficiency, and scalability compared to traditional ANNs."
      ],
      "metadata": {
        "id": "36_GgS-0dBgp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2)ANSWER:-\n",
        "\n",
        "To improve the performance of a CNN for image classification, we can experiment with various hyperparameters such as the number of epochs, choice of optimizer, learning rate, and others. Here's a general approach to tuning hyperparameters and reporting the improved test accuracy:\n",
        "\n",
        "1. **Hyperparameter Search**: Perform a systematic search or use techniques like grid search or random search to explore different combinations of hyperparameters.\n",
        "\n",
        "2. **Cross-Validation**: Use cross-validation to assess the performance of each set of hyperparameters more reliably. This helps to mitigate the risk of overfitting to the validation set.\n",
        "\n",
        "3. **Monitor Training Progress**: Keep track of training and validation metrics (such as loss and accuracy) during training to identify any signs of overfitting or underfitting.\n",
        "\n",
        "4. **Early Stopping**: Implement early stopping to prevent overfitting by monitoring the validation loss and stopping training when it starts to increase.\n",
        "\n",
        "5. **Regularization**: Experiment with regularization techniques like dropout or weight decay to prevent overfitting.\n",
        "\n",
        "6. **Data Augmentation**: Augment the training data by applying transformations such as rotation, translation, scaling, and flipping. This helps to increase the diversity of the training data and improve generalization.\n",
        "\n",
        "7. **Model Architecture**: Experiment with different CNN architectures, such as varying the number of layers, filter sizes, and strides.\n",
        "\n",
        "8. **Batch Size**: Adjust the batch size used during training, as larger batch sizes can sometimes lead to faster convergence but may also result in poorer generalization.\n",
        "\n",
        "Let's say you've performed these steps and found the optimal set of hyperparameters that improved the test accuracy. You can then report the improved test accuracy along with the chosen hyperparameters. For example:\n",
        "\n",
        "```python\n",
        "print(\"Improved Test Accuracy: 95.7%\")\n",
        "print(\"Hyperparameters:\")\n",
        "print(\"- Optimizer: Adam\")\n",
        "print(\"- Learning Rate: 0.001\")\n",
        "print(\"- Number of Epochs: 20\")\n",
        "print(\"- Dropout Rate: 0.2\")\n",
        "```\n",
        "\n",
        "In this hypothetical scenario, the test accuracy improved to 95.7% with the specified hyperparameters. These hyperparameters include the choice of optimizer (Adam), learning rate (0.001), number of epochs (20), and dropout rate (0.2)."
      ],
      "metadata": {
        "id": "zgNXQhzIdPND"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3)ANSWER:-\n",
        "\n",
        "Reducing the number of convolutional layers to only 1 in a Convolutional Neural Network (CNN) can have several effects on the network's performance and its ability to learn complex features from the input data:\n",
        "\n",
        "1. **Feature Learning**: With fewer convolutional layers, the network's capacity to learn hierarchical features from the input data decreases. Deep CNN architectures typically leverage multiple convolutional layers to capture low-level features in the early layers and progressively learn higher-level features in deeper layers. By reducing the number of convolutional layers to 1, the network might struggle to learn complex features.\n",
        "\n",
        "2. **Expressiveness**: Deep CNNs with multiple convolutional layers can model more complex relationships within the data due to the increased depth. Having only 1 convolutional layer limits the expressiveness of the network, potentially leading to suboptimal performance, especially on tasks that require modeling intricate patterns in the data.\n",
        "\n",
        "3. **Representation Power**: Deep CNNs are known for their ability to automatically learn hierarchical representations of the input data. Each convolutional layer captures different levels of abstraction, enabling the network to understand increasingly complex patterns. With only 1 convolutional layer, the network might not be able to capture enough abstraction, resulting in limited representation power.\n",
        "\n",
        "4. **Feature Reuse**: Deeper CNN architectures facilitate feature reuse, where learned features in earlier layers are combined and refined in subsequent layers. By reducing the number of convolutional layers to 1, the network loses the opportunity to leverage feature reuse, potentially leading to a less efficient use of learned features.\n",
        "\n",
        "5. **Capacity to Learn**: The capacity of the network to learn from the data is reduced with fewer convolutional layers. Deep CNNs often have a larger number of parameters, allowing them to learn intricate patterns and relationships. With only 1 convolutional layer, the network's capacity to learn may be insufficient for certain tasks, leading to lower performance.\n",
        "\n",
        "Overall, reducing the number of convolutional layers to only 1 can significantly impact the performance and effectiveness of a CNN, particularly on tasks that require modeling complex and hierarchical relationships in the input data, such as image classification and object detection."
      ],
      "metadata": {
        "id": "5Wu9fdo7dpdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4)ANSWER:-\n",
        "\n",
        "In the context of CNNs for image classification, the Softmax activation function is typically used in the last layer of the network when the task involves multi-class classification. Softmax converts the raw scores (logits) produced by the network into probabilities, where each output represents the likelihood of the input belonging to a particular class.\n",
        "\n",
        "However, when using a CNN for binary classification (e.g., distinguishing between two classes), it's common to use a different activation function in the last layer. The choice of activation function depends on the specific requirements of the task and the output representation desired.\n",
        "\n",
        "For binary classification tasks in CNNs, the most commonly used activation function in the last layer is the Sigmoid activation function. The Sigmoid function squashes the output into the range [0, 1], representing the probability of the input belonging to the positive class. This is suitable for binary classification tasks where the goal is to discriminate between two classes.\n",
        "\n",
        "Using Softmax in the last layer of a CNN for binary classification would still produce valid results, but it's less common and might not be as efficient or appropriate. Softmax is more suitable when dealing with multiple classes, where each class probability needs to be calculated independently. In binary classification, where there are only two classes, the output of a single neuron with a Sigmoid activation function can represent the probability of the positive class directly."
      ],
      "metadata": {
        "id": "7Zy4AGZEdya5"
      }
    }
  ]
}